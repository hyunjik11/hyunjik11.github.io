
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
  <head>
    <link
    rel="shortcut icon"
    sizes="16x16 32x32 64x64"
    href="./pictures/icon_head.png"
    />
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <style type="text/css" media="all">
      body {font-family: 'Droid Sans', helvetica,Arial,sans-serif;}
      a:link, a:visited, a:active {text-decoration:none}
      div.container{width:80%; margin:2%; line-height:140%;}
      div.right{float:right;width:200px; margin:1em; padding:1em;}
      div.content{margin-left:4%; padding:1em;}
      div.foo p {margin-bottom:0.0em; margin-top:0.0em;}
    </style>

    <title>Hyunjik Kim</title> 
    <meta name="description" content="Academic page of Hyunjik Kim. Research on deep learning.">
    <meta name="keywords" content="Deep learning, Machine learning">
    <meta name="resource-type" content="document">
    <meta name="distribution" content="global">
    <meta http-equiv="Content-Style-Type" content="text/css">

    <script type="text/javascript" async="" src="www.google-analytics.com/ga.js">
    </script>
    <script type="text/javascript">
        function trackOutboundLink(link, category, action) { 
            try { 
            _gaq.push(['_trackEvent', category , action]); 
            } catch(err){}
        }
    </script>

    <script type="text/javascript">
      var _gaq = _gaq || [];
      _gaq.push(['_setAccount', 'UA-8635368-2']);
      _gaq.push(['_trackPageview']);

      (function() {
        var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
        ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
      })();
    </script>
  </head>

  <body>
    <br>
    <div class="container">
      <div class="right"><img alt="" width="250px" src="./pictures/profile_head.jpg"></div>
      <div class="content">
        <h1 style="text-align: left;">Hyunjik Kim</h1>

        <b><font size="4">
          <a href="#preprints"      onClick="_gaq.push(['_trackEvent', 'withinpage', 'preprints', 'textlink']);">Preprints</a>
          | <a href="#publications" onclick="_gaq.push(['_trackEvent', 'withinpage', 'publications', 'textlink']);">Publications</a>
          </font>
        </b>

        <br>
        <br>

        <p>
          I'm a research scientist at  <a href="https://deepmind.com/">DeepMind</a> at the Google London office, working on various topics in Deep Learning.
          Prior to that I did my PhD in machine learning at the <a href="http://www.ox.ac.uk/">University of Oxford</a>, supervised by <a href="http://www.stats.ox.ac.uk/~teh/">Prof. Yee Whye Teh</a> in the <a href="http://csml.stats.ox.ac.uk/">Machine Learning group</a> at the <a href="http://stats.ox.ac.uk/"> Department of Statistics</a>. 
        </p>
        <p>
          Broadly speaking, my research interests lie in the field of probabilistic modelling and deep learning, mostly at the intersection of the two.
          My narrower research interests keep on evolving, and currently I'm interested in implicit neural representations and group equivariant neural networks.
          Prior to that, I worked on theoretical properties of self-attention, unsupervised representation learning (disentangling) and learning stochastic processes via Deep Learning methods (neural processes).
          I have also worked on scaling up inference for Gaussian processes, in particular on regression models for collaborative filtering that are motivated by a scalable approximation to a GP, 
          as well as a method for scaling up the compositional kernel search used by the <a href="https://www.automaticstatistician.com/index/">Automatic Statistician</a> via variational sparse GP methods.
        </p>
        <p> 
          Before my PhD, I studied Mathematics at the <a href="https://www.cam.ac.uk/">University of Cambridge</a>, from which I obtained B.A. and M.Math. degrees. 
          I spent a summer at <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-cambridge/">Microsoft Research, Cambridge</a> as a research intern, and worked on collaborative filtering. 
          I also spent a summer interning at <a href="https://deepmind.com/">DeepMind</a> working on unsupervised learning of disentangled representations.
        </p>
        <p>
          <a href="./cv/CV_Hyunjik_Kim.pdf" onclick="_gaq.push(['_trackEvent', 'downloads', 'resume', 'pdf']);">Curriculum Vitae</a> (last updated: July 2020)
        </p>
        <p>
          <a href="https://scholar.google.co.uk/citations?user=vxU3Zk4AAAAJ&hl=en&oi=ao">Google scholar page</a>
        </p>
          
        <p> 
        E-mail: <a href="mailto:hyunjikk@google.com">hyunjikk@google.com</a>
        </p>

        <div class="foo">
          <!-- Recent -->
            
          <H2><a id="recent">Recent</a></H2>
          <table border="0" cellspacing="10" cellpadding="2">
            <tbody>
                 <tr>
                  <td valign="top">
                    <b>From data to functa: Your data point is a function and you should treat it like one</b>
                    <p>
                    <em>Abstract:</em> It is common practice in deep learning to represent a measurement of the world on a discrete grid, e.g. a 2D grid of pixels. 
                      However, the underlying signal represented by these measurements is often continuous, e.g. the scene depicted in an image. 
                      A powerful continuous alternative is then to represent these measurements using an implicit neural representation, 
                      a neural function trained to output the appropriate measurement value for any input spatial location. 
                      In this paper, we take this idea to its next level: what would it take to perform deep learning on these functions instead, treating them as data? 
                      In this context we refer to the data as functa, and propose a framework for deep learning on functa. 
                      This view presents a number of challenges around efficient conversion from data to functa, compact representation of functa, and effectively solving downstream tasks on functa. 
                      We outline a recipe to overcome these challenges and apply it to a wide range of data modalities including images, 3D shapes, neural radiance fields (NeRF) and data on manifolds. 
                      We demonstrate that this approach has various compelling properties across data modalities, in particular on the canonical tasks of generative modeling, data imputation, novel view synthesis and classification.
                    </p>
                      <a href="https://emiliendupont.github.io/" onclick="trackOutboundLink(this, 'outbound', 'Emilien');">Emilien Dupont<sup>*</sup></a>,
                      <a href="./index.html" onclick="trackOutboundLink(this, 'outbound', 'Hyunjik');">Hyunjik Kim<sup>*</sup></a>,
                      <a href="http://arkitus.com/research/" onclick="trackOutboundLink(this, 'outbound', 'Ali Eslami');">Ali Eslami</a>,
                      <a href="https://danilorezende.com/" onclick="trackOutboundLink(this, 'outbound', 'Danilo Rezende');">Danilo Rezende</a>,
                      <a href="http://www.cs.huji.ac.il/~danrsm/" onclick="trackOutboundLink(this, 'outbound', 'Dan Rosenbaum');">Dan Rosenbaum</a>
                    <br>
                    <sup>*</sup>Equal contribution.<br>
                    <em>ArXiv</em>, 2022.<br>
                      <a href="https://arxiv.org/pdf/2201.12204" onclick="_gaq.push(['_trackEvent', 'downloads', 'functa', 'pdf' ]);">pdf</a>
                      | <a href="./papers/bibtex/functa.bib" onclick="_gaq.push(['_trackEvent', 'downloads', 'functa', 'bibtex' ]);">bibtex</a>
                    <br>
                  </td>
                </tr>

            </tbody>
          </table>			

          <!-- Publications -->
            
          <H2><a id="publications">Publications</a></H2>
          <table border="0" cellspacing="10" cellpadding="2">
            <tbody>
             <tr>
                <td valign="top">
                  <b>Group Equivariant Subsampling</b>
                  <p>
                  <em>Abstract:</em> Subsampling is used in convolutional neural networks (CNNs) in the form of pooling or strided convolutions, 
                    to reduce the spatial dimensions of feature maps and to allow the receptive fields to grow exponentially with depth. 
                    However, it is known that such subsampling operations are not translation equivariant, unlike convolutions that are translation equivariant. 
                    Here, we first introduce translation equivariant subsampling/upsampling layers that can be used to construct exact translation equivariant CNNs. 
                    We then generalise these layers beyond translations to general groups, thus proposing group equivariant subsampling/upsampling. 
                    We use these layers to construct group equivariant autoencoders (GAEs) that allow us to learn low-dimensional equivariant representations. 
                    We empirically verify on images that the representations are indeed equivariant to input translations and rotations, 
                    and thus generalise well to unseen positions and orientations. 
                    We further use GAEs in models that learn object-centric representations on multi-object datasets, 
                    and show improved data efficiency and decomposition compared to non-equivariant baselines.
                  </p>
                    <a href="https://jinxu06.github.io/" onclick="trackOutboundLink(this, 'outbound', 'Jin');">Jin Xu</a>,
                    <a href="./index.html" onclick="trackOutboundLink(this, 'outbound', 'Hyunjik');">Hyunjik Kim</a>,
                    <a href="https://www.robots.ox.ac.uk/~twgr/" onclick="trackOutboundLink(this, 'outbound', 'Tom');">Tom Rainforth</a>
                    <a href="http://www.stats.ox.ac.uk/~teh/" onclick="trackOutboundLink(this, 'outbound', 'Yee Whye');">Yee Whye Teh</a>
                  <br>
                  <em><b>NeurIPS 2021.</b></em> <br>
                    <a href="https://arxiv.org/pdf/2106.05886.pdf" onclick="_gaq.push(['_trackEvent', 'downloads', 'group_equivariant_subsampling', 'pdf' ]);">pdf</a>
                    | <a href="./papers/bibtex/group_equivariant_subsampling.bib" onclick="_gaq.push(['_trackEvent', 'downloads', 'group_equivariant_subsampling', 'bibtex' ]);">bibtex</a>
                  <br>
                </td>
              </tr>	
              
              <tr>
                <td valign="top">
                  <b>LieTransformer: Equivariant Self-Attention for Lie Groups</b>
                  <p>
                  <em>Abstract:</em> Group equivariant neural networks are used as building blocks of group invariant neural networks, 
                    which have been shown to improve generalisation performance and data efficiency through principled parameter sharing. 
                    Such works have mostly focused on group equivariant convolutions, 
                    building on the result that group equivariant linear maps are necessarily convolutions. 
                    In this work, we extend the scope of the literature to non-linear neural network modules, namely self-attention, 
                    that is emerging as a prominent building block of deep learning models. We propose the LieTransformer, 
                    an architecture composed of LieSelfAttention layers that are equivariant to arbitrary Lie groups and their discrete subgroups. 
                    We demonstrate the generality of our approach by showing experimental results that are competitive to baseline methods on a wide range of tasks: 
                    shape counting on point clouds, molecular property regression and modelling particle trajectories under Hamiltonian dynamics.
                  </p>
                    <a href="https://mjhutchinson.github.io/" onclick="trackOutboundLink(this, 'outbound', 'Michael');">Michael Hutchinson<sup>*</sup></a>,
                    <a href="http://csml.stats.ox.ac.uk/people/lelan/" onclick="trackOutboundLink(this, 'outbound', 'Charline');">Charline Le Lan<sup>*</sup></a>,
                    <a href="https://shehzaidi.github.io/" onclick="trackOutboundLink(this, 'outbound', 'Sheh');">Sheheryar Zaidi<sup>*</sup></a>,
                    <a href="https://emiliendupont.github.io/" onclick="trackOutboundLink(this, 'outbound', 'Emilien');">Emilien Dupont</a>,
                    <a href="http://www.stats.ox.ac.uk/~teh/" onclick="trackOutboundLink(this, 'outbound', 'Yee Whye');">Yee Whye Teh</a>,
                    <a href="./index.html" onclick="trackOutboundLink(this, 'outbound', 'Hyunjik');">Hyunjik Kim</a>
                  <br>
                  <sup>*</sup>Equal contribution.<br>                  
                  <em><b>ICML 2021.</b></em> <br>
                    <a href="https://arxiv.org/pdf/2012.10885" onclick="_gaq.push(['_trackEvent', 'downloads', 'lietransformer', 'pdf' ]);">pdf</a>
                    | <a href="./papers/bibtex/lietransformer.bib" onclick="_gaq.push(['_trackEvent', 'downloads', 'lietransformer', 'bibtex' ]);">bibtex</a>
                    | <a href="https://github.com/oxcsml/lie-transformer" onclick="_gaq.push(['_trackEvent', 'downloads', 'lietransformer', 'code' ]);">github</a>
                  <br>
                </td>
              </tr>	

              <tr>
                <td valign="top">
                  <b>The Lipschitz Constant of Self-Attention</b>
                  <p>
                  <em>Abstract:</em> Lipschitz constants of neural networks have been explored in various contexts in deep learning, 
                    such as provable adversarial robustness, estimating Wasserstein distance, stabilising training of GANs, 
                    and formulating invertible neural networks. Such works have focused on bounding the Lipschitz constant of 
                    fully connected or convolutional networks, composed of linear maps and pointwise non-linearities. 
                    In this paper, we investigate the Lipschitz constant of self-attention, a non-linear neural network module 
                    widely used in sequence modelling. We prove that the standard dot-product self-attention is not Lipschitz, 
                    and propose an alternative L2 self-attention that is Lipschitz. We derive an upper bound on the 
                    Lipschitz constant of L2 self-attention and provide empirical evidence for its asymptotic tightness. 
                    To demonstrate the practical relevance of the theory, we formulate invertible self-attention and 
                    use it in a Transformer-based architecture for a character-level language modelling task.
                  </p>
                    <a href="./index.html" onclick="trackOutboundLink(this, 'outbound', 'Hyunjik');">Hyunjik Kim</a>,
                    <a href="https://gpapamak.github.io/" onclick="trackOutboundLink(this, 'outbound', 'George');">George Papamakarios</a>,
                    <a href="https://www.cs.toronto.edu/~amnih/" onclick="trackOutboundLink(this, 'outbound', 'Andriy');">Andriy Mnih</a>
                  <br>
                  <em><b>ICML 2021.</b></em> <br>
                    <a href="https://arxiv.org/pdf/2006.04710" onclick="_gaq.push(['_trackEvent', 'downloads', 'lipschitz', 'pdf' ]);">pdf</a>
                    | <a href="./papers/bibtex/lipschitz.bib" onclick="_gaq.push(['_trackEvent', 'downloads', 'lipschitz', 'bibtex' ]);">bibtex</a>
                  <br>
                </td>
              </tr>
              <tr>
                <td valign="top">
                  <b>MetaFun: Meta-Learning with Iterative Functional Updates</b>
                  <p>
                  <em>Abstract:</em> We develop a functional encoder-decoder approach to supervised meta-learning, 
                    where labeled data is encoded into an infinite-dimensional functional representation rather 
                    than a finite-dimensional one. Furthermore, rather than directly producing the representation, 
                    we learn a neural update rule resembling functional gradient descent which iteratively improves the representation. 
                    The final representation is used to condition the decoder to make predictions on unlabeled data. 
                    Our approach is the first to demonstrates the success of encoder-decoder style meta-learning methods 
                    like conditional neural processes on large-scale few-shot classification benchmarks such as miniImageNet
                    and tieredImageNet, where it achieves state-of-the-art performance.
                  </p>
                    <a href="https://jinxu06.github.io/" onclick="trackOutboundLink(this, 'outbound', 'Jin');">Jin Xu</a>,
                    <a href="https://savior287.github.io/JFT-webpage/" onclick="trackOutboundLink(this, 'outbound', 'Jeff');">Jean-Francois Ton</a>,
                    <a href="./index.html" onclick="trackOutboundLink(this, 'outbound', 'Hyunjik');">Hyunjik Kim</a>,
                    <a href="http://akosiorek.github.io/" onclick="trackOutboundLink(this, 'outbound', 'Adam');">Adam Kosiorek</a>,
                    <a href="http://www.stats.ox.ac.uk/~teh/" onclick="trackOutboundLink(this, 'outbound', 'Yee Whye');">Yee Whye Teh</a>
                  <br>
                  <em><b>ICML 2020.</b></em> <br>
                    <a href="https://arxiv.org/pdf/1912.02738.pdf" onclick="_gaq.push(['_trackEvent', 'downloads', 'metafun', 'pdf' ]);">pdf</a>
                    | <a href="./papers/bibtex/metafun.bib" onclick="_gaq.push(['_trackEvent', 'downloads', 'metafun', 'bibtex' ]);">bibtex</a>
                  <br>	
                </td>
              </tr>

              <tr>
                <td valign="top">
                  <b>Attentive Neural Processes</b>
                  <p>
                  <em>Abstract:</em> Neural Processes (NPs) (Garnelo et al., 2018a,b) approach regression by learning
                  to map a context set of observed input-output pairs to a distribution over regression
                  functions. Each function models the distribution of the output given an input, conditioned
                  on the context. NPs have the benefit of fitting observed data efficiently
                  with linear complexity in the number of context input-output pairs, and can learn
                  a wide family of conditional distributions; they learn predictive distributions conditioned
                  on context sets of arbitrary size. Nonetheless, we show that NPs suffer a
                  fundamental drawback of underfitting, giving inaccurate predictions at the inputs
                  of the observed data they condition on. We address this issue by incorporating
                  attention into NPs, allowing each input location to attend to the relevant context
                  points for the prediction. We show that this greatly improves the accuracy of predictions,
                  results in noticeably faster training, and expands the range of functions
                  that can be modelled.
                  </p>
                    <a href="./index.html" onclick="trackOutboundLink(this, 'outbound', 'Hyunjik');">Hyunjik Kim</a>,
                    <a href="https://www.cs.toronto.edu/~amnih/" onclick="trackOutboundLink(this, 'outbound', 'Andriy');">Andriy Mnih</a>,
                    <a href="https://jonathan-schwarz.github.io/" onclick="trackOutboundLink(this, 'outbound', 'Jonathan');">Jonathan Schwarz</a>,
                    <a href="https://www.doc.ic.ac.uk/~mg4413/" onclick="trackOutboundLink(this, 'outbound', 'Marta');">Marta Garnelo</a>,
                    <a href="http://arkitus.com/research/" onclick="trackOutboundLink(this, 'outbound', 'Ali Eslami');">Ali Eslami</a>,
                    <a href="http://www.cs.huji.ac.il/~danrsm/" onclick="trackOutboundLink(this, 'outbound', 'Dan Rosenbaum');">Dan Rosenbaum</a>,
                    <a href="https://ai.google/research/people/OriolVinyals" onclick="trackOutboundLink(this, 'outbound', 'Oriol Vinyals');">Oriol Vinyals</a>,
                    <a href="http://www.stats.ox.ac.uk/~teh/" onclick="trackOutboundLink(this, 'outbound', 'Yee Whye');">Yee Whye Teh</a>
                  <br>
                  <em>Bayesian Deep Learning Workshop, NeurIPS 2018. </em> Contributed Talk.<br>
                  <a href="http://bayesiandeeplearning.org/2018/papers/28.pdf" onclick="_gaq.push(['_trackEvent', 'downloads', 'anp', 'pdf' ]);">pdf</a>
                  <br>
                  <em><b>ICLR 2019.</b></em> <br>
                    <a href="https://openreview.net/pdf?id=SkE6PjC9KX" onclick="_gaq.push(['_trackEvent', 'downloads', 'anp', 'pdf' ]);">pdf</a>
                    | <a href="./papers/bibtex/anp.bib" onclick="_gaq.push(['_trackEvent', 'downloads', 'anp', 'bibtex' ]);">bibtex</a> 
                    | <a href="https://openreview.net/forum?id=SkE6PjC9KX" onclick="_gaq.push(['_trackEvent', 'downloads', 'anp', 'openreview' ]);">openreview</a> 
                    | <a href="https://github.com/deepmind/neural-processes" onclick="_gaq.push(['_trackEvent', 'downloads', 'anp', 'github' ]);">github</a> 
                  <br>
                </td>
              </tr>
                
              <tr>
                <td valign="top">
                  <b>Sequential Attend, Infer, Repeat: Generative Modelling of Moving Objects</b>
                  <p>
                  <em>Abstract:</em> We present Sequential Attend, Infer, Repeat (SQAIR), an interpretable deep generative model for videos of moving objects. 
                    It can reliably discover and track objects throughout the sequence of frames, 
                    and can also generate future frames conditioning on the current frame, 
                    thereby simulating expected motion of objects. 
                    This is achieved by explicitly encoding object presence, locations and appearances in the latent variables of the model. 
                    SQAIR retains all strengths of its predecessor, Attend, Infer, Repeat (AIR, Eslami et. al., 2016), 
                    including learning in an unsupervised manner, and addresses its shortcomings. 
                    We use a moving multi-MNIST dataset to show limitations of AIR in detecting overlapping or partially occluded objects,
                    and show how SQAIR overcomes them by leveraging temporal consistency of objects. 
                    Finally, we also apply SQAIR to real-world pedestrian CCTV data, 
                    where it learns to reliably detect, track and generate walking pedestrians with no supervision.

                  </p>
                    <a href="http://akosiorek.github.io/" onclick="trackOutboundLink(this, 'outbound', 'Adam');">Adam Kosiorek</a>,
                    <a href="./index.html" onclick="trackOutboundLink(this, 'outbound', 'Hyunjik');">Hyunjik Kim</a>,
                    <a href="http://ori.ox.ac.uk/mrg_people/ingmar-posner/" onclick="trackOutboundLink(this, 'outbound', 'Ingmar');">Ingmar Posner</a>,
                    <a href="http://www.stats.ox.ac.uk/~teh/" onclick="trackOutboundLink(this, 'outbound', 'Yee Whye');">Yee Whye Teh</a>
                  <br>
                  <em><b>NeurIPS 2018, </em> Spotlight.</b><br>
                    <a href="https://arxiv.org/abs/1806.01794" onclick="_gaq.push(['_trackEvent', 'downloads', 'sqair', 'pdf' ]);">pdf</a>
                    | <a href="./papers/bibtex/sqair.bib" onclick="_gaq.push(['_trackEvent', 'downloads', 'sqair', 'bibtex' ]);">bibtex</a>
                    | <a href="https://github.com/akosiorek/sqair" onclick="_gaq.push(['_trackEvent', 'downloads', 'sqair', 'github' ]);">github</a>
                  <br>
                </td>
              </tr>

              <tr>
                <td valign="top">
                  <b>Disentangling by Factorising</b>
                  <p>
                  <em>Abstract:</em> We define and address the problem of unsupervised learning of disentangled representations on data generated from independent factors of variation. We propose FactorVAE, a method that disentangles by encouraging the distribution of representations to be factorial and hence independent across the dimensions. We show that it improves upon β-VAE by providing a better trade-off between disentanglement and reconstruction quality. 
                    Moreover, we highlight the problems of a commonly used disentanglement metric and introduce a new metric that does not suffer from them.</p>
                    <a href="./index.html" onclick="trackOutboundLink(this, 'outbound', 'Hyunjik');">Hyunjik Kim</a>,
                    <a href="https://www.cs.toronto.edu/~amnih/" onclick="trackOutboundLink(this, 'outbound', 'Andriy');">Andriy Mnih</a>
                  <br>
                  <em>Learning Disentangled Representations: From Perception to Control Workshop, NIPS 2017.</em> Spotlight Talk. <br>
                    <a href="https://drive.google.com/open?id=0Bwy4Nlx78QCCQzJldThTUlJfal9ha0l3bThRZXY3RmFCUVB3" onclick="_gaq.push(['_trackEvent', 'downloads', 'factorvae', 'pdf' ]);">pdf</a>
                  <br>
                  <em><b>ICML 2018.</b></em> <br>
                    <a href="https://arxiv.org/pdf/1802.05983.pdf" onclick="_gaq.push(['_trackEvent', 'downloads', 'disent', 'pdf' ]);">pdf</a>
                    | <a href="./papers/bibtex/disentangling_by_factorising.bib" onclick="_gaq.push(['_trackEvent', 'downloads', 'disent', 'bibtex' ]);">bibtex</a>
                  <br>	
                </td>
              </tr>

                
              <tr>
                <td valign="top">
                  <b>Scaling up the Automatic Statistician: Scalable Structure Discovery using Gaussian Processes</b>
                  <p>
                  <em>Abstract:</em> Automating statistical modelling is a challenging problem in artificial intelligence. 
                    The Automatic Statistician takes a first step in this direction, by employing a kernel search algorithm with Gaussian Processes (GP) to provide interpretable statistical models for regression problems. 
                    However this does not scale due to its O(N^3) running time for the model selection. 
                    We propose Scalable Kernel Composition (SKC), a scalable kernel search algorithm that extends the Automatic Statistician to bigger data sets. 
                    In doing so, we derive a cheap upper bound on the GP marginal likelihood that sandwiches the marginal likelihood with the variational lower bound. 
                    We show that the upper bound is significantly tighter than the lower bound and thus useful for model selection.</p>
                    <a href="./index.html" onclick="trackOutboundLink(this, 'outbound', 'Hyunjik');">Hyunjik Kim</a>,
                    <a href="http://www.stats.ox.ac.uk/~teh/" onclick="trackOutboundLink(this, 'outbound', 'Yee Whye');">Yee Whye Teh</a>
                  <br>	
                  <em>AutoML 2016, Journal of Machine Learning Research Workshop and Conference Proceedings.</em> <br>
                    <em>Practical Bayesian Nonparametrics Workshop, NIPS 2016.</em> Oral &amp; Travel Award. <br>
                    <a href="http://www.jmlr.org/proceedings/papers/v64/kim_scalable_2016.pdf" onclick="_gaq.push(['_trackEvent', 'downloads', 'ssdgp', 'pdf' ]);">pdf</a>
                  <br>
                  <em><b>AISTATS 2018,</em> Oral.</b><br>
                    <a href="https://arxiv.org/pdf/1706.02524" onclick="_gaq.push(['_trackEvent', 'downloads', 'tuckergp', 'pdf' ]);">pdf</a>
                    | <a href="./papers/bibtex/ssdgp.bib" onclick="_gaq.push(['_trackEvent', 'downloads', 'tuckergp', 'bibtex' ]);">bibtex</a>
                  <br>
                </td>
              </tr>
              <tr>
                <td height="10px"></td>
              </tr>
            </tbody>
          </table>
            
          <!-- Preprints -->
          <H2><a id="preprints">Preprints</a></H2>
            <table border="0" cellspacing="10" cellpadding="2">
              <tbody>
                <tr>
                  <td valign="top">
                    <b>Meta-Learning surrogate models for sequential decision making</b>
                    <p>
                    <em>Abstract:</em> We introduce a unified probabilistic framework for solving sequential decision making problems 
                      ranging from Bayesian optimisation to contextual bandits and reinforcement learning. This is accomplished 
                      by a probabilistic model-based approach that explains observed data while capturing predictive uncertainty 
                      during the decision making process. Crucially, this probabilistic model is chosen to be a Meta-Learning system 
                      that allows learning from a distribution of related problems, allowing data efficient adaptation to a target task. 
                      As a suitable instantiation of this framework, we explore the use of Neural processes due to statistical and 
                      computational desiderata. We apply our framework to a broad range of problem domains, such as control problems, 
                      recommender systems and adversarial attacks on RL agents, demonstrating an efficient and general black-box learning 
                      approach.
                    </p>
                      <a href="https://jonathan-schwarz.github.io/" onclick="trackOutboundLink(this, 'outbound', 'Jonathan');">Jonathan Schwarz</a>,
                      <a href="http://galashov.com/" onclick="trackOutboundLink(this, 'outbound', 'Alexandre');">Alexandre Galashov</a>,	
                      <a href="./index.html" onclick="trackOutboundLink(this, 'outbound', 'Hyunjik');">Hyunjik Kim</a>,
                      <a href="https://www.doc.ic.ac.uk/~mg4413/" onclick="trackOutboundLink(this, 'outbound', 'Marta');">Marta Garnelo</a>,
                      <a href="https://scholar.google.com/citations?user=ekterPoAAAAJ" onclick="trackOutboundLink(this, 'outbound', 'David Saxton');">David Saxton</a>,
                      <a href="https://sites.google.com/site/pushmeet/" onclick="trackOutboundLink(this, 'outbound', 'Pushmeet');">Pushmeet Kohli</a>,	
                      <a href="http://arkitus.com/research/" onclick="trackOutboundLink(this, 'outbound', 'Ali Eslami');">Ali Eslami</a>,
                      <a href="http://www.stats.ox.ac.uk/~teh/" onclick="trackOutboundLink(this, 'outbound', 'Yee Whye');">Yee Whye Teh</a>
                    <br>
                    <em>ArXiv</em>, 2019.<br>
                      <a href="https://arxiv.org/pdf/1903.11907" onclick="_gaq.push(['_trackEvent', 'downloads', 'surrogate', 'pdf' ]);">pdf</a>
                      | <a href="./papers/bibtex/surrogate.bib" onclick="_gaq.push(['_trackEvent', 'downloads', 'surrogate', 'bibtex' ]);">bibtex</a>
                    <br>
                  </td>
                </tr>
                  
                <tr>
                  <td valign="top">
                    <b>Collaborative Filtering with Side Information: a Gaussian Process Perspective</b>
                    <p>
                    <em>Abstract:</em> We tackle the problem of collaborative filtering (CF) with side information,
                      through the lens of Gaussian Process (GP) regression. Driven by the idea of using the kernel 
                      to explicitly model user-item similarities, we formulate the GP in a way that allows the 
                      incorporation of low-rank matrix factorisation, arriving at our model, the Tucker Gaussian Process (TGP). 
                      Consequently, TGP generalises classical Bayesian matrix factorisation models, 
                      and goes beyond them to give a natural and elegant method for incorporating side information, 
                      giving enhanced predictive performance for CF problems. 
                      Moreover we show that it is a novel model for regression, 
                      especially well-suited to grid-structured data and problems 
                      where the dependence on covariates is close to being separable.
                    </p>
                      <a href="./index.html" onclick="trackOutboundLink(this, 'outbound', 'Hyunjik');">Hyunjik Kim</a>,
                      Xiaoyu Lu,
                      <a href="http://sethrf.com/" onclick="trackOutboundLink(this, 'outbound', 'Seth');">Seth Flaxman</a>,
                      <a href="http://www.stats.ox.ac.uk/~teh/" onclick="trackOutboundLink(this, 'outbound', 'Yee Whye');">Yee Whye Teh</a>
                    <br>
                    <em>ArXiv</em>, 2016.<br>
                      <a href="https://arxiv.org/pdf/1605.07025" onclick="_gaq.push(['_trackEvent', 'downloads', 'tuckergp', 'pdf' ]);">pdf</a>
                      | <a href="./papers/bibtex/tuckergp.bib" onclick="_gaq.push(['_trackEvent', 'downloads', 'tuckergp', 'bibtex' ]);">bibtex</a>
                    <br>
                  </td>
                </tr>
                <tr>
                  <td height="10px"></td>
                </tr>
              </tbody>
            </table>

          <!-- Talks -->
          <H2><a id="talks">Talks</a></H2>
          <table border="0" cellspacing="10" cellpadding="2">
            <tbody>

              <tr>
                <td valign="top">
                  <b>Topics on Attention in Deep Learning </b>
                  <p>
                  <em>Abstract:</em> Attention mechanisms are being widely used in state-of-the-art deep learning models across various data modalities. 
                    In this talk, we explore the concept of attention or self-attention from two perspectives: 
                    1. A methodological point of view and 2. A theoretical point of view. 
                    For 1, we study the Attentive Neural Process (ANP) that incorporates attention into the recently introduced Neural Process (NP), 
                    a deep neural network that learns a stochastic process, with applications in meta-learning. 
                    We show the role of attention in ANPs that allows it to address some fundamental drawbacks of NPs. 
                    For 2, we investigate the Lipschitz constant of self-attention, that measures how much the output of self-attention can change with respect to the change in its inputs. 
                    We thus theoretically demonstrate how self-attention is different to standard neural network architectures such as fully connected networks and convolutional networks.
                  </p>
                  <em>Venue:</em> <a href="http://ai.postech.ac.kr/semina/view/page/1/id/57#u">Summer AI Seminar Series @POSTECH</a>, 06/08/20.
                  <br> <a href="./talks/Topics_on_Attention_in_Deep_Learning.pdf">slides</a>
                </td>
              </tr>
                
              <tr>
                <td valign="top">
                  <b>Attention: the Analogue of Kernels in Deep Learning </b>
                  <p>
                  <em>Abstract:</em> There have been many recent works that lie at the intersection of kernel methods and deep learning, 
                    namely Deep Kernel Learning, Deep Gaussian Processes (GPs) and Convolutional GPs. However such works are often 
                    motivated by borrowing ideas that originate from deep learning and incorporating them into kernel methods. 
                    In this talk, we will explore the concept of attention or self-attention, that has interestingly travelled the
                    opposite path; it is inherently motivated from kernels, but is being used extensively in state-of-the-art deep
                    learning models in various data modalities. We investigate attention in more detail by studying the 
                    Attentive Neural Process (ANP) that incorporates attention into the recently introduced Neural Process (NP), 
                    a deep models that learns a stochastic process. We show that ANPs address some fundamental drawbacks of NPs 
                    by bringing them closer to GPs, while maintaining the benefits of neural networks such as scalability and flexibility.
                  </p>
                  <em>Venue:</em> <a href="https://sites.google.com/corp/view/kernel-workshop/">Recent Developments in Kernel Methods workshop @Gatsby Computational Neuroscience Unit, UCL</a>, 27/09/19.
                  <br> <a href="./talks/Attention_the_Analogue_of_Kernels_in_Deep_Learning.pdf">slides</a>
                </td>
              </tr>

              <tr>
                <td valign="top">
                  <b>Interpretable Models in Probabilistic Deep Learning </b>
                  <p>
                  <em>Abstract:</em> As Deep Learning (DL) solutions to real-world problems are becoming increasingly common, DL researchers 
                    are striving to better understand the models that they develop. The community has been using the term 
                    ‘interpretability’ to describe models and methods that help us achieve this rather vague goal. However many 
                    claim that deep models are inherently uninterpretable due to their black-box nature, and stop paying attention to 
                    interpretability in deep models on these grounds. In this talk, we show that ‘deep’ and ‘interpretability’ are not 
                    mutually exclusive terms, hence it is both possible and necessary to devise interpretable deep models. We first 
                    clarify what is meant by the term ‘interpretability’, by listing its desiderata and properties. We then introduce 
                    examples of deep probabilistic models that enjoy various properties of interpretability: the talk will cover 
                    FactorVAE, a model for learning disentangled representations, and the Attentive Neural Process, a model for learning 
                    stochastic processes in a data-driven fashion, focusing on their applications to image data.
                  </p>
                  <em>Venues:</em> <a href="https://imrc.kist.re.kr/en/"> Korea Institute of Science and Technology (KIST), Center for Imaging Media Research</a>, 03/04/19.
                  <br> <a href="https://www.naverlabs.com/"> Naver Labs</a>, 04/04/19.	
                  <br> <a href="https://cv.snu.ac.kr/"> Seoul National University, Computer Vision Lab</a>, 05/04/19.
                  <br> <a href="./talks/Interpretable_Models_in_Probabilistic_Deep_Learning.pdf">slides</a>
                </td>
              </tr>
            </tbody>
          </table>	
            
            
          <!-- Public Engagement -->
          <H2><a id="public engagement">Public Engagement</a></H2>
          <table border="0" cellspacing="10" cellpadding="2">
            <tbody>
              <tr>
                <td valign="top">
                  <b>Introducing Machine Learning to the Public</b>
                  <p>
                  I helped create a cute two-minute animation that introduces machine learning to the general public, along with friends at Oxford.
                  Check it out below!
                  <br> <br>
                  <iframe width="560" height="315" src="https://www.youtube.com/embed/f_uwKZIAeM0" frameborder="0" allowfullscreen></iframe>
                  <br><br>
                  Further details can be found <a href="http://www.oxfordsparks.ox.ac.uk/content/what-machine-learning" onclick="_gaq.push(['_trackEvent', 'downloads', 'oxfordsparks']);">here</a>.
                  </p>
                </td>
              </tr>	
            </tbody>
          </table>
        </div>
      </div>
    </div>

  </body>
</html>
